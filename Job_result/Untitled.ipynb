{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b4b505c-25dd-44ab-b1ce-b42fbe6a7aa8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538683d4-3b5f-46ff-8278-24fe07c1603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908d3049-b2a1-40e4-83f5-384394602e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33edb963-334a-4d69-a44a-c1858cfcf52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob as glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8ef3571-e828-483d-8794-f95b3d0a71a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8626636d-56e6-4a3b-8bc0-368a52e734b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_28049_2_csv = pd.read_csv('./train_28002.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4367cdf8-b8fd-427d-b321-7bcbf05d7061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>mask_rle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_5425_10</td>\n",
       "      <td>./train_img_crop/TRAIN_5425_10.png</td>\n",
       "      <td>247 12 470 13 695 12 717 1 919 12 1143 13 1160...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_2224_19</td>\n",
       "      <td>./train_img_crop/TRAIN_2224_19.png</td>\n",
       "      <td>66 22 290 22 514 22 738 22 962 22 1186 22 1411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_1157_19</td>\n",
       "      <td>./train_img_crop/TRAIN_1157_19.png</td>\n",
       "      <td>92 6 193 28 319 2 417 26 642 23 866 21 1091 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_6761_25</td>\n",
       "      <td>./train_img_crop/TRAIN_6761_25.png</td>\n",
       "      <td>32 40 170 20 256 40 394 20 480 40 618 20 704 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_3521_9</td>\n",
       "      <td>./train_img_crop/TRAIN_3521_9.png</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          img_id                            img_path  \\\n",
       "0  TRAIN_5425_10  ./train_img_crop/TRAIN_5425_10.png   \n",
       "1  TRAIN_2224_19  ./train_img_crop/TRAIN_2224_19.png   \n",
       "2  TRAIN_1157_19  ./train_img_crop/TRAIN_1157_19.png   \n",
       "3  TRAIN_6761_25  ./train_img_crop/TRAIN_6761_25.png   \n",
       "4   TRAIN_3521_9   ./train_img_crop/TRAIN_3521_9.png   \n",
       "\n",
       "                                            mask_rle  \n",
       "0  247 12 470 13 695 12 717 1 919 12 1143 13 1160...  \n",
       "1  66 22 290 22 514 22 738 22 962 22 1186 22 1411...  \n",
       "2  92 6 193 28 319 2 417 26 642 23 866 21 1091 19...  \n",
       "3  32 40 170 20 256 40 394 20 480 40 618 20 704 4...  \n",
       "4                                                 -1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_28049_2_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca456e19-ec7e-40eb-b259-ec633a471288",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_path = train_28049_2_csv.img_path\n",
    "y = train_28049_2_csv.mask_rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c05485b-51a1-45ab-bb9d-893ba0f5c2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = train_test_split(dataset, test_size = .1, random_state=42, shuffle=True)\n",
    "train_dataloader = Dataloader(train_dataset , ...)\n",
    "valid_dataloader = Dataloader(data_dataset, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "748beab3-1a07-4e55-9d21-31e6e540cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_path, y, test_size=.1, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decf404c-6f0a-4b36-9351-cf02c495b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97227a-52bd-41f4-b017-bc6814b4d19d",
   "metadata": {},
   "source": [
    "# 정환이꺼 ^_^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de0c084b-c0c6-4203-bc1e-4fcb7c3fe733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.losses import DiceLoss, TverskyLoss, FocalLoss, LovaszLoss\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from run_length_encoding import *\n",
    "from load import *\n",
    "from loss import *\n",
    "from dacon_dice import *\n",
    "\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea2121e-d89d-46dc-92b8-c3b8845ae71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\"  # Set the GPUs 0 and 1 to use\n",
    "\n",
    "\n",
    "#get gpu DEVICE\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델 구성 별로 지정 필요.\n",
    "SEED = 18 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "042decab-5d51-4ffd-9af3-d9692ad7a609",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECTURE = 'UnetPlusPlus' # UnetPlusPlus, DeepLabV3, DeepLabV3Plus\n",
    "ENCODER = 'timm-efficientnet-b7' # resnet152\n",
    "ENCODER_WEIGHT= 'noisy-student' # 1: imagenet\n",
    "N_CLASSES = 1\n",
    "ACTIVATION = None\n",
    "OPTIMIZER = 'AdamW'\n",
    "SAVED_MODEL_PATH = '/root/jupyter/Dacon/deeplabv3p/model_save_{}_{}_4/'.format(ARCHITECTURE,ENCODER)\n",
    "\n",
    "# Train Parameters\n",
    "TRAIN_DATA_CSV = './train_28002.csv'\n",
    "BATCH_SIZE = 48 # 2GPUs Maximum\n",
    "VALID_SET_RATIO = .1\n",
    "START_EPOCH = 1 # 고정 \n",
    "NUM_EPOCH = 30\n",
    "LOSS_PATH = \"./loss_history/\" # 고정\n",
    "LEARNING_RATE = 1e-4 \n",
    "WEIGHT_DECAY = 5.0e-02\n",
    "\n",
    "INF = float('inf') # 고정\n",
    "tol = 1e-6 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a7d106-5218-491b-b093-3146bdea4fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create directory /root/jupyter/Dacon/deeplabv3p/model_save_UnetPlusPlus_timm-efficientnet-b7_4/\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(SAVED_MODEL_PATH):\n",
    "    print('create directory {}'.format(SAVED_MODEL_PATH))\n",
    "    os.mkdir(SAVED_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bfd5786-4986-49d2-adfa-497709d1fc63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no saved model\n"
     ]
    }
   ],
   "source": [
    "is_weight = get_weight(SAVED_MODEL_PATH)\n",
    "if is_weight == False:\n",
    "    print('there is no saved model')\n",
    "    model = get_model(ARCHITECTURE)\n",
    "    model = model(classes=N_CLASSES,\n",
    "                encoder_name=ENCODER,\n",
    "                encoder_weights=ENCODER_WEIGHT,\n",
    "                activation=ACTIVATION)\n",
    "    \n",
    "    model = nn.DataParallel(model) \n",
    "    model.to(DEVICE)\n",
    "else:\n",
    "    last_epoch, last_ckpt = is_weight\n",
    "    print('last epoch is {}'.format(last_epoch))\n",
    "    print('model-{} loaded..'.format(last_epoch))\n",
    "    model = get_model(ARCHITECTURE)\n",
    "    model = model(classes=N_CLASSES,\n",
    "                encoder_name=ENCODER,\n",
    "                encoder_weights=ENCODER_WEIGHT,\n",
    "                activation=ACTIVATION)\n",
    "    \n",
    "    model = nn.DataParallel(model) \n",
    "    model.load_state_dict(last_ckpt, strict=False)\n",
    "    model.to(DEVICE)\n",
    "    START_EPOCH = last_epoch+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b332cf-e359-4159-b022-073b0c2daac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(OPTIMIZER)\n",
    "optimizer = optimizer(model.parameters(),lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "# Transform\n",
    "transform_train = get_transform_for_train(version=4) # version4 fix\n",
    "transform_valid = get_transform_for_test()\n",
    "\n",
    "train_dataloader, validation_dataloader = train_valid_seed(csv_file=TRAIN_DATA_CSV,\n",
    "                                                      transform_train=transform_train,\n",
    "                                                      transform_valid=transform_valid,\n",
    "                                                      batch_size=BATCH_SIZE,\n",
    "                                                      test_size=VALID_SET_RATIO,\n",
    "                                                     random_seed=SEED,\n",
    "                                                     shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fda63e4-6412-4d19-8b4e-867547b45122",
   "metadata": {},
   "outputs": [],
   "source": [
    "bceLoss = torch.nn.BCEWithLogitsLoss()\n",
    "dice_loss = DiceLoss(mode='binary')\n",
    "\n",
    "pasted_epoch_score = [INF] \n",
    "pasted_dice_loss = []\n",
    "pasted_bce_loss = []\n",
    "pasted_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d06cb645-7b15-4ae0-b18a-0963b023aeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 526/526 [05:35<00:00,  1.57it/s]\n",
      " 15% 9/59 [00:10<00:55,  1.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mDEVICE, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     44\u001b[0m msks \u001b[38;5;241m=\u001b[39m msks\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mDEVICE, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m---> 45\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m dc_sc \u001b[38;5;241m=\u001b[39m calculate_dice_scores_from_rle(outputs,msks)\n\u001b[1;32m     48\u001b[0m epoch_score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dc_sc\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/parallel_apply.py:81\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     79\u001b[0m         thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m---> 81\u001b[0m         \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     _worker(\u001b[38;5;241m0\u001b[39m, modules[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m], kwargs_tup[\u001b[38;5;241m0\u001b[39m], devices[\u001b[38;5;241m0\u001b[39m], streams[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:1011\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1028\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(START_EPOCH, START_EPOCH+NUM_EPOCH):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    epoch_bce_loss = 0\n",
    "    epoch_dice_loss = 0\n",
    "    epoch_train_loss = 0\n",
    "    \n",
    "    for imgs, msks in tqdm(train_dataloader):\n",
    "        imgs = imgs.to(device=DEVICE, dtype = torch.float)\n",
    "        msks = msks.to(device=DEVICE, dtype = torch.float)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        \n",
    "        \n",
    "        #diceloss = dice_loss(outputs, msks.unsqueeze(1))\n",
    "        bceloss = bceLoss(outputs, msks.unsqueeze(1))\n",
    "        diceloss = dice_loss(outputs, msks.unsqueeze(1))\n",
    "        \n",
    "        w1 = .7\n",
    "        w2 = .4\n",
    "        loss = w1 * bceloss + w2 * diceloss\n",
    "        \n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_bce_loss += bceloss.item()\n",
    "        epoch_dice_loss += diceloss.item()\n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "    pasted_bce_loss.append(epoch_bce_loss/len(train_dataloader))\n",
    "    pasted_dice_loss.append(epoch_dice_loss/len(train_dataloader))\n",
    "    pasted_loss.append(epoch_train_loss/len(train_dataloader))\n",
    "    \n",
    "    epoch_score = 0\n",
    "    #val_asl_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        result = []\n",
    "        for imgs,msks in tqdm(validation_dataloader):\n",
    "            imgs = imgs.to(device=DEVICE, dtype = torch.float)\n",
    "            msks = msks.to(device=DEVICE, dtype = torch.float)\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            dc_sc = calculate_dice_scores_from_rle(outputs,msks)\n",
    "            epoch_score += dc_sc\n",
    "            \n",
    "            \n",
    "            #val_asl = validation_asl(outputs, msks.unsqueeze(1))\n",
    "            #val_asl_loss += val_asl.item()\n",
    "    \n",
    "    \n",
    "    print(f'Epoch {epoch}')\n",
    "    print(f'BCE Loss: {epoch_bce_loss/len(train_dataloader)}')\n",
    "    print(f'DICE Loss: {epoch_dice_loss/len(train_dataloader)}')\n",
    "    print(f'Total Loss: {epoch_train_loss/len(train_dataloader)}')\n",
    "    \n",
    "    #print(f'Validation Asymmetric Loss: {val_asl_loss/len(validation_dataloader)}')\n",
    "    print(f'Validation Dice Score: {epoch_score/len(validation_dataloader)}')\n",
    "    \n",
    "    pasted_epoch_score.append(epoch_score/len(validation_dataloader))\n",
    "    #pasted_val_loss.append(val_asl_loss/len(validation_dataloader))\n",
    "    \n",
    "    # save a weight every epoch\n",
    "    path = SAVED_MODEL_PATH + '{}_{}-{num:0004d}.pth'.format(ARCHITECTURE,ENCODER,num=epoch)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    \n",
    "    if np.abs(pasted_epoch_score[-2] - pasted_epoch_score[-1])< tol:\n",
    "        print('Early Stop')\n",
    "        break;\n",
    "    \n",
    "# save epoch losses as .csv\n",
    "loss_n_score = [pasted_bce_loss, pasted_dice_loss, pasted_epoch_score[1:]]\n",
    "loss_df = pd.DataFrame(loss_n_score)\n",
    "loss_df = loss_df.transpose()\n",
    "# !!!!파일명 변경 필요!!!!\n",
    "loss_df.to_csv(path_or_buf=LOSS_PATH + '{}-{}_4'.format(ARCHITECTURE, ENCODER)+'.csv', \n",
    "               index=False,\n",
    "               header=['train_BCELoss','train_DiceLoss','val_DiceScore'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3108b2-cbbc-4418-9dee-70fdef81d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "*** 변경사항 ***\n",
    "SAVED_MODEL_PATH -> model_save_{ARCHITECTURE}_{ENCODER}/ 로 자동 지정 및 자동 생성.\n",
    "SEED -> architecture, encoder등의 변경 사항이 있을 경우 변경 필요.\n",
    "        모든 파라미터가 동일한 경우에 대해 이어서 학습 할 경우 동일한 SEED사용.\n",
    "get_transform_for_train(), get_transform_for_test() -> load.py\n",
    "get_dataset(), get_dataloader(),random_split_train_valid() -> load.train_valid_seed()로 변경.\n",
    "\n",
    "*** 주의 사항 ***\n",
    "모델이 같아도, 다른 paramters 변경 할 경우 이전 모델 학습 이어서 불가.\n",
    "SAVED_MODEL_PATH 따로 지정 해주어야 함.\n",
    "아래 쪽에 csv파일 저장되는 부분이랑 model weight저장되는 부분 한번 더 확인하고 실행.\n",
    "\n",
    "*** 사용 방법 ***\n",
    "해당 파일을 copy and paste.\n",
    "파일 명 변경 -> train_ARCHITECTURE_ENCODER.py\n",
    "model params들을 변경해준다.\n",
    "SEED 변경해준다.\n",
    " \n",
    "\n",
    "'''\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.losses import DiceLoss, TverskyLoss, FocalLoss, LovaszLoss\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from run_length_encoding import *\n",
    "from load import *\n",
    "from loss import *\n",
    "from dacon_dice import *\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "#gpu setting\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\"  # Set the GPUs 0 and 1 to use\n",
    "\n",
    "\n",
    "#get gpu DEVICE\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델 구성 별로 지정 필요.\n",
    "SEED = 18 \n",
    "\n",
    "# Model Parameters\n",
    "ARCHITECTURE = 'UnetPlusPlus' # UnetPlusPlus, DeepLabV3, DeepLabV3Plus\n",
    "ENCODER = 'timm-efficientnet-b7' # resnet152\n",
    "ENCODER_WEIGHT= 'noisy-student' # 1: imagenet\n",
    "N_CLASSES = 1\n",
    "ACTIVATION = None\n",
    "OPTIMIZER = 'AdamW'\n",
    "SAVED_MODEL_PATH = '/root/jupyter/Dacon/deeplabv3p/model_save_{}_{}_3/'.format(ARCHITECTURE,ENCODER)\n",
    "\n",
    "# Train Parameters\n",
    "TRAIN_DATA_CSV = './train_28002.csv'\n",
    "BATCH_SIZE = 48 # 2GPUs Maximum\n",
    "VALID_SET_RATIO = .1\n",
    "START_EPOCH = 1 # 고정 \n",
    "NUM_EPOCH = 30\n",
    "LOSS_PATH = \"./loss_history/\" # 고정\n",
    "LEARNING_RATE = 1e-4 \n",
    "WEIGHT_DECAY = 5.0e-02\n",
    "\n",
    "# ASL loss\n",
    "GAMMA_NEG=3\n",
    "GAMMA_POS=1\n",
    "\n",
    "# Others\n",
    "INF = float('inf') # 고정\n",
    "tol = 1e-6 \n",
    "\n",
    "\n",
    "# 모델 저장 경로(SAVED_MODEL_PATH) 확인 및 디렉토리 생성\n",
    "if not os.path.exists(SAVED_MODEL_PATH):\n",
    "    print('create directory {}'.format(SAVED_MODEL_PATH))\n",
    "    os.mkdir(SAVED_MODEL_PATH)\n",
    "    \n",
    "\n",
    "\n",
    "#load weight\n",
    "is_weight = get_weight(SAVED_MODEL_PATH)\n",
    "if is_weight == False:\n",
    "    print('there is no saved model')\n",
    "    model = get_model(ARCHITECTURE)\n",
    "    model = model(classes=N_CLASSES,\n",
    "                encoder_name=ENCODER,\n",
    "                encoder_weights=ENCODER_WEIGHT,\n",
    "                activation=ACTIVATION)\n",
    "    \n",
    "    model = nn.DataParallel(model) \n",
    "    model.to(DEVICE)\n",
    "else:\n",
    "    last_epoch, last_ckpt = is_weight\n",
    "    print('last epoch is {}'.format(last_epoch))\n",
    "    print('model-{} loaded..'.format(last_epoch))\n",
    "    model = get_model(ARCHITECTURE)\n",
    "    model = model(classes=N_CLASSES,\n",
    "                encoder_name=ENCODER,\n",
    "                encoder_weights=ENCODER_WEIGHT,\n",
    "                activation=ACTIVATION)\n",
    "    \n",
    "    model = nn.DataParallel(model) \n",
    "    model.load_state_dict(last_ckpt, strict=False)\n",
    "    model.to(DEVICE)\n",
    "    START_EPOCH = last_epoch+1\n",
    "\n",
    "    \n",
    "                                                 \n",
    "# OPTIMIZER \n",
    "optimizer = get_optimizer(OPTIMIZER)\n",
    "optimizer = optimizer(model.parameters(),lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "# Transform\n",
    "transform_train = get_transform_for_train(version=4) # version4 fix\n",
    "transform_valid = get_transform_for_test()\n",
    "\n",
    "train_dataloader, validation_dataloader = train_valid_seed(csv_file=TRAIN_DATA_CSV,\n",
    "                                                      transform_train=transform_train,\n",
    "                                                      transform_valid=transform_valid,\n",
    "                                                      batch_size=BATCH_SIZE,\n",
    "                                                      test_size=VALID_SET_RATIO,\n",
    "                                                     random_seed=SEED,\n",
    "                                                     shuffle=True)\n",
    "\n",
    "\n",
    "# LOSS\n",
    "#bceLoss = torch.nn.BCEWithLogitsLoss()\n",
    "#dice_loss = DiceLoss(mode='binary')\n",
    "#iouloss = IoULoss()\n",
    "asl_loss = AsymmetricLoss(gamma_neg=GAMMA_NEG, gamma_pos=GAMMA_POS, clip=0.05, disable_torch_grad_focal_loss=True)\n",
    "#lovasz\n",
    "tversky_loss = TverskyLoss(mode='binary',alpha=0.7, beta=0.3)\n",
    "\n",
    "\n",
    "\n",
    "# LISTs of Train Losses and Valid Score\n",
    "pasted_epoch_score = [INF] \n",
    "pasted_val_loss = []\n",
    "pasted_tversky_loss = []\n",
    "pasted_asl_loss = []\n",
    "pasted_loss = []\n",
    "\n",
    "\n",
    "for epoch in range(START_EPOCH, START_EPOCH+NUM_EPOCH):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    epoch_asl_loss = 0\n",
    "    epoch_tversky_loss = 0\n",
    "    epoch_train_loss = 0\n",
    "    \n",
    "    for imgs, msks in tqdm(train_dataloader):\n",
    "        imgs = imgs.to(device=DEVICE, dtype = torch.float)\n",
    "        msks = msks.to(device=DEVICE, dtype = torch.float)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        \n",
    "        \n",
    "        #diceloss = dice_loss(outputs, msks.unsqueeze(1))\n",
    "        tverskyloss = tversky_loss(outputs, msks.unsqueeze(1))\n",
    "        aslloss = asl_loss(outputs, msks.unsqueeze(1))\n",
    "        \n",
    "        loss = aslloss + tverskyloss\n",
    "        \n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_asl_loss += aslloss.item()\n",
    "        epoch_tversky_loss += tverskyloss.item()\n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "    pasted_asl_loss.append(epoch_asl_loss/len(train_dataloader))\n",
    "    pasted_tversky_loss.append(epoch_tversky_loss/len(train_dataloader))\n",
    "    pasted_loss.append(epoch_train_loss/len(train_dataloader))\n",
    "    \n",
    "    epoch_score = 0\n",
    "    val_asl_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        result = []\n",
    "        for imgs,msks in tqdm(validation_dataloader):\n",
    "            imgs = imgs.to(device=DEVICE, dtype = torch.float)\n",
    "            msks = msks.to(device=DEVICE, dtype = torch.float)\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            dc_sc = calculate_dice_scores_from_rle(outputs,msks)\n",
    "            epoch_score += dc_sc\n",
    "            \n",
    "            \n",
    "            val_asl = validation_asl(outputs, msks.unsqueeze(1))\n",
    "            val_asl_loss += val_asl.item()\n",
    "    \n",
    "    \n",
    "    print(f'Epoch {epoch}')\n",
    "    print(f'Asymmetric Loss: {epoch_asl_loss/len(train_dataloader)}')\n",
    "    print(f'Tversky Loss: {epoch_tversky_loss/len(train_dataloader)}')\n",
    "    print(f'Total Loss: {epoch_train_loss/len(train_dataloader)}')\n",
    "    \n",
    "    print(f'Validation Asymmetric Loss: {val_asl_loss/len(validation_dataloader)}')\n",
    "    print(f'Validation Dice Score: {epoch_score/len(validation_dataloader)}')\n",
    "    \n",
    "    pasted_epoch_score.append(epoch_score/len(validation_dataloader))\n",
    "    pasted_val_loss.append(val_asl_loss/len(validation_dataloader))\n",
    "    \n",
    "    # save a weight every epoch\n",
    "    path = SAVED_MODEL_PATH + '{}_{}-{num:0004d}.pth'.format(ARCHITECTURE,ENCODER,num=epoch)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    \n",
    "    if np.abs(pasted_epoch_score[-2] - pasted_epoch_score[-1])< tol:\n",
    "        print('Early Stop')\n",
    "        break;\n",
    "    \n",
    "# save epoch losses as .csv\n",
    "loss_n_score = [pasted_asl_loss, pasted_dice_loss, pasted_loss, pasted_val_loss, pasted_epoch_score[1:]]\n",
    "loss_df = pd.DataFrame(loss_n_score)\n",
    "loss_df = loss_df.transpose()\n",
    "# !!!!파일명 변경 필요!!!!\n",
    "loss_df.to_csv(path_or_buf=LOSS_PATH + '{}-{}_3'.format(ARCHITECTURE, ENCODER)+'.csv', \n",
    "               index=False,\n",
    "               header=['train ASLoss','train TverskyLoss','train TotalLoss','val ASLoss','val_DiceScore'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eccee8c-7c9c-4da1-810f-a88af7cadd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf54942-8d72-420d-be74-336a9b9eb56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('train_52857.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48278d90-6753-4fe9-a4fe-9cde01e8877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.drop(columns='Unnamed: 0').reset_index(drop = True).sample(frac= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc1df2c7-d7e4-46d4-9c7e-091fc45162fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>mask_rle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42701</th>\n",
       "      <td>TRAIN_0234_20</td>\n",
       "      <td>./train_img_crop/TRAIN_0234_20.png</td>\n",
       "      <td>8 121 191 34 232 121 415 34 456 121 639 34 680...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21002</th>\n",
       "      <td>TRAIN_6778_5</td>\n",
       "      <td>./train_img_crop/TRAIN_6778_5.png</td>\n",
       "      <td>19948 1 19956 1 20171 6 20395 9 20619 12 20842...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3327</th>\n",
       "      <td>TRAIN_0257_13</td>\n",
       "      <td>./train_img_crop/TRAIN_0257_13.png</td>\n",
       "      <td>17 19 241 19 465 19 686 1 689 19 913 19 1137 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37641</th>\n",
       "      <td>TRAIN_5681_17</td>\n",
       "      <td>./train_img_crop/TRAIN_5681_17.png</td>\n",
       "      <td>212 13 436 13 660 13 884 13 1108 13 1322 23 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5283</th>\n",
       "      <td>TRAIN_0682_12</td>\n",
       "      <td>./train_img_crop/TRAIN_0682_12.png</td>\n",
       "      <td>158 38 206 19 382 38 430 19 606 38 654 19 830 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22225</th>\n",
       "      <td>TRAIN_3503_19</td>\n",
       "      <td>./train_img_crop/TRAIN_3503_19.png</td>\n",
       "      <td>162 34 386 35 609 36 833 35 1058 34 1284 32 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>TRAIN_6454_6</td>\n",
       "      <td>./train_img_crop/TRAIN_6454_6.png</td>\n",
       "      <td>217 8 442 7 666 7 890 7 1115 6 1339 6 1564 5 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42505</th>\n",
       "      <td>TRAIN_5276_17</td>\n",
       "      <td>./train_img_crop/TRAIN_5276_17.png</td>\n",
       "      <td>12852 40 13076 40 13300 40 13524 40 13748 40 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>TRAIN_3167_15</td>\n",
       "      <td>./train_img_crop/TRAIN_3167_15.png</td>\n",
       "      <td>12444 37 12668 37 12892 37 13116 37 13340 37 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46148</th>\n",
       "      <td>TRAIN_3087_2</td>\n",
       "      <td>./train_img_crop/TRAIN_3087_2.png</td>\n",
       "      <td>10521 8 10745 8 10969 8 11193 8 11417 8 11648 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52857 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              img_id                            img_path  \\\n",
       "42701  TRAIN_0234_20  ./train_img_crop/TRAIN_0234_20.png   \n",
       "21002   TRAIN_6778_5   ./train_img_crop/TRAIN_6778_5.png   \n",
       "3327   TRAIN_0257_13  ./train_img_crop/TRAIN_0257_13.png   \n",
       "37641  TRAIN_5681_17  ./train_img_crop/TRAIN_5681_17.png   \n",
       "5283   TRAIN_0682_12  ./train_img_crop/TRAIN_0682_12.png   \n",
       "...              ...                                 ...   \n",
       "22225  TRAIN_3503_19  ./train_img_crop/TRAIN_3503_19.png   \n",
       "15995   TRAIN_6454_6   ./train_img_crop/TRAIN_6454_6.png   \n",
       "42505  TRAIN_5276_17  ./train_img_crop/TRAIN_5276_17.png   \n",
       "167    TRAIN_3167_15  ./train_img_crop/TRAIN_3167_15.png   \n",
       "46148   TRAIN_3087_2   ./train_img_crop/TRAIN_3087_2.png   \n",
       "\n",
       "                                                mask_rle  \n",
       "42701  8 121 191 34 232 121 415 34 456 121 639 34 680...  \n",
       "21002  19948 1 19956 1 20171 6 20395 9 20619 12 20842...  \n",
       "3327   17 19 241 19 465 19 686 1 689 19 913 19 1137 1...  \n",
       "37641  212 13 436 13 660 13 884 13 1108 13 1322 23 15...  \n",
       "5283   158 38 206 19 382 38 430 19 606 38 654 19 830 ...  \n",
       "...                                                  ...  \n",
       "22225  162 34 386 35 609 36 833 35 1058 34 1284 32 15...  \n",
       "15995  217 8 442 7 666 7 890 7 1115 6 1339 6 1564 5 1...  \n",
       "42505  12852 40 13076 40 13300 40 13524 40 13748 40 1...  \n",
       "167    12444 37 12668 37 12892 37 13116 37 13340 37 1...  \n",
       "46148  10521 8 10745 8 10969 8 11193 8 11417 8 11648 ...  \n",
       "\n",
       "[52857 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2444cbf-83be-4f80-8a1b-f94c1598ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.to_csv('train_52857.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930115b8-ac67-404d-baf6-3e373e5d966e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
