{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d276ad6b-bf73-412f-9bb9-bc2df0d30ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_loss = {}\n",
    "past_loss['BCE'] = []\n",
    "past_loss['DICE'] = []\n",
    "for i in range(100):\n",
    "    past_loss['BCE'].append(i)\n",
    "    past_loss['DICE'].append(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "546b8c6a-5f20-42fc-985d-dca533e92b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(past_loss['BCE']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49083b11-8349-4947-b829-2fdae3b91de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.losses import DiceLoss, TverskyLoss, FocalLoss, LovaszLoss\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from run_length_encoding import *\n",
    "from load import *\n",
    "from loss import *\n",
    "from dacon_dice import *\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4ec386-b0b3-4ee4-837c-9800c5111b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\"  # Set the GPUs 0 and 1 to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9dfd8c1-508c-43ed-9d4d-93fc1868b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get gpu DEVICE\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델 구성 별로 지정 필요.\n",
    "SEED = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85807bc4-1e0a-42cb-a22b-644a4852d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ARCHITECTURE = 'DeepLabV3Plus' # UnetPlusPlus, DeepLabV3, DeepLabV3Plus\n",
    "ENCODER = 'resnet152' # resnet152\n",
    "ENCODER_WEIGHT= 'imagenet' # 1: imagenet\n",
    "N_CLASSES = 2\n",
    "ACTIVATION = None\n",
    "OPTIMIZER = 'AdamW'\n",
    "SAVED_MODEL_PATH = '/root/jupyter/Dacon/deeplabv3p/model_save_{}_{}_prac/'.format(ARCHITECTURE,ENCODER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf9d029-1186-4db8-8ba4-8441401b8226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Parameters\n",
    "TRAIN_DATA_CSV = './train_52857.csv'\n",
    "BATCH_SIZE = 160 # 2GPUs Maximum\n",
    "VALID_SET_RATIO = .1\n",
    "START_EPOCH = 1 # 고정 \n",
    "NUM_EPOCH = 30\n",
    "LOSS_PATH = \"./loss_history/\" # 고정\n",
    "LEARNING_RATE = 1e-4 \n",
    "WEIGHT_DECAY = 5.0e-02\n",
    "\n",
    "INF = float('inf') # 고정\n",
    "tol = 1e-5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ddb2d50-3e57-4018-930b-2f36bc1b38b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create directory /root/jupyter/Dacon/deeplabv3p/model_save_DeepLabV3Plus_resnet152_prac/\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(SAVED_MODEL_PATH):\n",
    "    print('create directory {}'.format(SAVED_MODEL_PATH))\n",
    "    os.mkdir(SAVED_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a66d0a73-1332-425c-8a06-9f2a3ef0f709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no saved model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "is_weight = get_weight(SAVED_MODEL_PATH)\n",
    "if is_weight == False:\n",
    "    print('there is no saved model')\n",
    "    model = get_model(ARCHITECTURE)\n",
    "    model = model(classes=N_CLASSES,\n",
    "                encoder_name=ENCODER,\n",
    "                encoder_weights=ENCODER_WEIGHT,\n",
    "                activation=ACTIVATION)\n",
    "    \n",
    "    model = nn.DataParallel(model) \n",
    "    model.to(DEVICE)\n",
    "else:\n",
    "    last_epoch, last_ckpt = is_weight\n",
    "    print('last epoch is {}'.format(last_epoch))\n",
    "    print('model-{} loaded..'.format(last_epoch))\n",
    "    model = get_model(ARCHITECTURE)\n",
    "    model = model(classes=N_CLASSES,\n",
    "                encoder_name=ENCODER,\n",
    "                encoder_weights=ENCODER_WEIGHT,\n",
    "                activation=ACTIVATION)\n",
    "    \n",
    "    model = nn.DataParallel(model) \n",
    "    model.load_state_dict(last_ckpt, strict=False)\n",
    "    model.to(DEVICE)\n",
    "    START_EPOCH = last_epoch+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8678dc1-1a60-43be-bb3a-5829e7dffd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = get_optimizer(OPTIMIZER)\n",
    "optimizer = optimizer(model.parameters(),lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d2b140c-5b15-4f85-8e77-ffe972aa27f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = get_transform_for_train(version=4) # version4 fix\n",
    "transform_valid = get_transform_for_test()\n",
    "\n",
    "train_dataloader, validation_dataloader = train_valid_seed(csv_file=TRAIN_DATA_CSV,\n",
    "                                                      transform_train=transform_train,\n",
    "                                                      transform_valid=transform_valid,\n",
    "                                                      batch_size=BATCH_SIZE,\n",
    "                                                      test_size=VALID_SET_RATIO,\n",
    "                                                     random_seed=SEED,\n",
    "                                                     shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bad92d5-b939-4d6b-8175-65af435b9d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {}\n",
    "losses['BCE'] = torch.nn.BCEWithLogitsLoss()\n",
    "losses['DICE'] = DiceLoss(mode = 'binary')\n",
    "loss_weights = []\n",
    "loss_weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "081cef49-e8fa-4a5e-9feb-0f8617f23d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasted_epoch_score = [INF] \n",
    "pasted_dice_loss = []\n",
    "pasted_bce_loss = []\n",
    "pasted_loss = []\n",
    "pasted_train_score = []\n",
    "pasted_val_loss = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee92866-c33f-4714-aad5-041246cde51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, train_dataloader, valid_dataloader, EPOCHES, losses, loss_weights):\n",
    "    past_loss = {}\n",
    "    for epoch in range(1, 1+EPOCHES):\n",
    "        model.train()\n",
    "        #def 함수로 변환\n",
    "        \n",
    "        epoch_info = {}\n",
    "        for loss in losses:\n",
    "            epoch_info[loss] = []\n",
    "        epoch_info['score'] = []\n",
    "        \n",
    "        \n",
    "        for imgs, msks in tqdm(train_dataloader):\n",
    "            imgs = imgs.to(device=DEVICE, dtype = torch.float)\n",
    "            msks = msks.to(device=DEVICE, dtype = torch.float)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            loss_values = []\n",
    "            loss = 0\n",
    "            for i, loss_func in enumerate(losses):\n",
    "                loss_frac = loss_weights[i] * losses[loss_func](outputs,msks.unsqueeze(1))\n",
    "                loss_values.append(loss_frac)\n",
    "                loss += loss_frac\n",
    "\n",
    "            dc_sc = calculate_dice_scores_from_rle(outputs,msks)    \n",
    "\n",
    "            epoch_info['score'].append(dc_sc)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch}')\n",
    "        print(f'Train_score = {np.array(epoch_info['score']).mean()}')\n",
    "        \n",
    "        for loss_func in enumerate(losses):\n",
    "            print(f'Train_{loss_func} = {np.array(epoch_info[loss_func]).mean()}')\n",
    "\n",
    "        pasted_bce_loss.append(epoch_bce_loss/len(train_dataloader))\n",
    "        pasted_dice_loss.append(epoch_dice_loss/len(train_dataloader))\n",
    "        pasted_loss.append(epoch_train_loss/len(train_dataloader))\n",
    "        pasted_train_score.append(epoch_train_score/len(train_dataloader))\n",
    "\n",
    "        epoch_score = 0\n",
    "        val_asl_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            result = []\n",
    "            for imgs,msks in tqdm(validation_dataloader):\n",
    "                imgs = imgs.to(device=DEVICE, dtype = torch.float)\n",
    "                msks = msks.to(device=DEVICE, dtype = torch.float)\n",
    "                outputs = model(imgs)\n",
    "\n",
    "                dc_sc = calculate_dice_scores_from_rle(outputs,msks)\n",
    "                epoch_score += dc_sc\n",
    "\n",
    "\n",
    "                val_asl = validation_asl(outputs, msks.unsqueeze(1))\n",
    "                val_asl_loss += val_asl.item()\n",
    "\n",
    "\n",
    "        print(f'Epoch {epoch}')\n",
    "        print(f'BCE Loss: {epoch_bce_loss/len(train_dataloader)}')\n",
    "        print(f'DICE Loss: {epoch_dice_loss/len(train_dataloader)}')\n",
    "        print(f'Total Loss: {epoch_train_loss/len(train_dataloader)}')\n",
    "        print(f'Total Score: {epoch_train_score/len(train_dataloader)}')\n",
    "        print(f'Validation Asymmetric Loss: {val_asl_loss/len(validation_dataloader)}')\n",
    "        print(f'Validation Dice Score: {epoch_score/len(validation_dataloader)}')\n",
    "\n",
    "        pasted_epoch_score.append(epoch_score/len(validation_dataloader))\n",
    "        pasted_val_loss.append(val_asl_loss/len(validation_dataloader))\n",
    "\n",
    "        # save a weight every epoch\n",
    "        path = SAVED_MODEL_PATH + '{}_{}-{num:0004d}.pth'.format(ARCHITECTURE,ENCODER,num=epoch)\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "        if np.abs(pasted_epoch_score[-2] - pasted_epoch_score[-1])< tol:\n",
    "            print('Early Stop')\n",
    "            break;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
